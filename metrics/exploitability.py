from abc import abstractmethod, ABC
from time import sleep
from omegaconf import DictConfig
from typing import Any, Dict, List, Type, Union
import numpy as np

from open_spiel.python import rl_agent, rl_environment
from open_spiel.python.algorithms import random_agent
from open_spiel.python.algorithms.exploitability import exploitability, nash_conv, best_response
from open_spiel.python.rl_agent_policy import JointRLAgentPolicy

from metrics.base_metric import Metric



class ExploitabilityMetric(Metric):
    """Computes the exploitability of each joint policy in the set of grouped agents (the set of joint policies)"""
    
    def __init__(self, 
            config : DictConfig,
            eval_frequency: int, 
            ) -> None:
        """Initialize the exploitability metric.
        
        Args:
            config (DictConfig): the configuration of the run
            eval_frequency (int): the frequency at which the agents are evaluated
        """
        self.eval_frequency = eval_frequency
        super().__init__(config = config)

    def evaluate(self, 
            group_names_to_grouped_agents : Dict[str, List[rl_agent.AbstractAgent]],
            group_names_to_envs : List[rl_environment.Environment],
            episode_idx : int,
            ) -> Dict[str, float]:
        """Computes the exploitability of each joint policy in the set of grouped agents (the set of joint policies)
                
        Args:
            group_names_to_grouped_agents (Dict[str, List[rl_agent.AbstractAgent]]): the agents to evaluate, grouped by group name
            group_names_to_envs (List[rl_environment.Environment]): the environments to evaluate the agents in
            episode_idx (int): the current episode index
            
        Returns:
            Dict[str, float]: a dictionary of the metrics to return at each evaluation step, with the metric names as keys and the metric values as values
        """

        if not episode_idx % self.eval_frequency == 0:
            return {}
        
        def get_nash_conv_metrics(grouped_agents : List[rl_agent.AbstractAgent], env : rl_environment.Environment) -> Dict[str, Any]:
            """This function computes the nash conv of a joint policy (a set of agents) in a given environment.
            
            Args:
                grouped_agents (List[rl_agent.AbstractAgent]): the agents to evaluate
                env (rl_environment.Environment): the environment to evaluate the agents in
            """
            metrics_dict = {}
            policy = JointRLAgentPolicy(
                game = env.game,
                agents={i: agent for i, agent in enumerate(grouped_agents)},
                use_observation=True,
                )
                        
            metrics_dict["nash_conv"], metrics_dict["player_improvements"] = nash_conv(
                game = env.game, 
                policy = policy,
                return_only_nash_conv=False,
                use_cpp_br=True,
                )

            return metrics_dict


        metrics_dict = {}
        
        for group_name, grouped_agents in group_names_to_grouped_agents.items():
            env = group_names_to_envs[group_name]
            # Compute Nash Conv metrics
            nash_conv_metrics = get_nash_conv_metrics(grouped_agents = grouped_agents, env = env)
            nash_conv_value = nash_conv_metrics["nash_conv"]
            player_improvements = nash_conv_metrics["player_improvements"]
            # Add the metrics to the metrics dictionary
            metrics_dict[f"nash_conv/{group_name}"] = nash_conv_value
            for player_id in range(len(player_improvements)):
                player_name = self.config["agents"][group_name][player_id]
                metrics_dict[f"nash_conv/{group_name}"] = nash_conv_value
                metrics_dict[f"nash_conv/{group_name} part of {player_name} (id={player_id})"] = player_improvements[player_id]

        return metrics_dict


